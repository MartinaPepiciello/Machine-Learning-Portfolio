{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping function\n",
    "Scrape articles' titles, summaries and URLs from the desired section of BBC News. The number of pages to load can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_bbcnews(base_url, n_pages=50):\n",
    "\n",
    "    # Navigate to the webpage\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Wait for me to close cookie overlays\n",
    "    time.sleep(7)\n",
    "\n",
    "    # Initialize variables\n",
    "    articles = []\n",
    "    titles = set()\n",
    "    counter = 0\n",
    "\n",
    "    try:\n",
    "        while counter < n_pages:      \n",
    "            # Wait for new articles to load\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, \"lx-stream\")))\n",
    "\n",
    "            # Find article containers\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            lx_stream_div = soup.find(\"div\", id=\"lx-stream\")\n",
    "            article_containers = lx_stream_div.find_all(\"li\", class_=\"lx-stream__post-container\")\n",
    "\n",
    "            for container in article_containers:\n",
    "                article = []\n",
    "                # Extract the title\n",
    "                title = container.find(\"header\", class_=\"lx-stream-post__header\")\n",
    "                if title:\n",
    "                    # Skip duplicate page if title was already present\n",
    "                    if title in titles:\n",
    "                        counter -= 1\n",
    "                        break\n",
    "                    else:\n",
    "                        titles.add(title)\n",
    "\n",
    "                    article.append(title.text.strip())\n",
    "                else:\n",
    "                    article.append(None)\n",
    "\n",
    "                # Extract the summary text\n",
    "                summary = container.find(\"p\", class_=\"lx-stream-related-story--summary\")\n",
    "                if summary:\n",
    "                    article.append(summary.text.strip())\n",
    "                else:\n",
    "                    article.append(None)\n",
    "\n",
    "                # Extract the URL\n",
    "                link = container.find(\"a\", class_=\"qa-story-cta-link\")\n",
    "                if link and 'href' in link.attrs:\n",
    "                    article.append(link['href'])\n",
    "                else:\n",
    "                    article.append(None) \n",
    "\n",
    "                if None not in article:\n",
    "                    articles.append(article)\n",
    "\n",
    "            # Attempt to find the \"Next\" button and exit if there is none\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CLASS_NAME, \"qa-pagination-next-page\")\n",
    "                next_button.click()\n",
    "            except ElementClickInterceptedException:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering function\n",
    "Scrapes the webpage, filters interesting articles and outputs an HTML file containing articles titles, summaries and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os\n",
    "\n",
    "def predict_bbcnews(website_link, vectorizer, model):\n",
    "    \n",
    "    n_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "\n",
    "    articles = scrape_bbcnews(website_link, n_pages)\n",
    "\n",
    "    interesting_articles = []\n",
    "\n",
    "    # Loop through the articles, predict their interestingness, and filter the interesting ones\n",
    "    for article in articles:\n",
    "        title, summary, link = article\n",
    "\n",
    "        # Vectorize the text\n",
    "        text = (title + ' ' + summary).lower()\n",
    "        X = vectorizer.transform([text])\n",
    "        \n",
    "        # Append article if interesting\n",
    "        if model.predict(X)[0]:\n",
    "            interesting_articles.append({'title': title, 'summary': summary, 'link': link})\n",
    "\n",
    "    # Generate HTML content for interesting articles\n",
    "    html_output = f'<html><head><title>Interesting articles from {website_link}</title></head><body><div style=\"width: 1000px; max-width: fit-content; margin: 20px auto\">'\n",
    "\n",
    "    for article in interesting_articles:\n",
    "        title = article['title']\n",
    "        summary = article['summary']\n",
    "        link = article['link']\n",
    "        \n",
    "        html_output += f'<h2><a href=\"https://www.bbc.com{link}\" target=\"_blank\">{title}</a></h2>'\n",
    "        html_output += f\"<p>{summary}</p>\"\n",
    "        html_output += '<br>'\n",
    "\n",
    "    html_output += \"</div></body></html>\"\n",
    "\n",
    "    # Save the HTML file and open it\n",
    "    output_filename = \"interesting_articles.html\"\n",
    "    with open(output_filename, 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(html_output)\n",
    "    webbrowser.open(os.path.abspath(output_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the filtering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_world_url = \"https://www.bbc.com/news/world\"\n",
    "bbc_science_url = \"https://www.bbc.com/news/science_and_environment\"\n",
    "bbc_tech_url = \"https://www.bbc.com/news/technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load(\"bbc_world_model.pkl\")\n",
    "vectorizer = joblib.load(\"bbc_world_vectorizer.pkl\")\n",
    "\n",
    "predict_bbcnews(bbc_world_url, vectorizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
