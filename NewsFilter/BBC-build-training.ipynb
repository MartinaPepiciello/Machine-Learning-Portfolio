{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping function\n",
    "Scrape articles' titles, summaries and URLs from the desired section of BBC News. The number of pages to load can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_bbcnews(base_url, n_pages):\n",
    "\n",
    "    # Navigate to the webpage\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Wait for me to close cookie overlays\n",
    "    time.sleep(7)\n",
    "\n",
    "    # Initialize variables\n",
    "    articles = []\n",
    "    titles = set()\n",
    "    counter = 0\n",
    "\n",
    "    try:\n",
    "        while counter < n_pages:      \n",
    "            # Wait for new articles to load\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, \"lx-stream\")))\n",
    "\n",
    "            # Find article containers\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            lx_stream_div = soup.find(\"div\", id=\"lx-stream\")\n",
    "            article_containers = lx_stream_div.find_all(\"li\", class_=\"lx-stream__post-container\")\n",
    "\n",
    "            for container in article_containers:\n",
    "                article = []\n",
    "                # Extract the title\n",
    "                title = container.find(\"header\", class_=\"lx-stream-post__header\")\n",
    "                if title:\n",
    "                    # Skip duplicate page if title was already present\n",
    "                    if title in titles:\n",
    "                        counter -= 1\n",
    "                        break\n",
    "                    else:\n",
    "                        titles.add(title)\n",
    "\n",
    "                    article.append(title.text.strip())\n",
    "                else:\n",
    "                    article.append(None)\n",
    "\n",
    "                # Extract the summary text\n",
    "                summary = container.find(\"p\", class_=\"lx-stream-related-story--summary\")\n",
    "                if summary:\n",
    "                    article.append(summary.text.strip())\n",
    "                else:\n",
    "                    article.append(None)\n",
    "\n",
    "                # Extract the URL\n",
    "                link = container.find(\"a\", class_=\"qa-story-cta-link\")\n",
    "                if link and 'href' in link.attrs:\n",
    "                    article.append(link['href'])\n",
    "                else:\n",
    "                    article.append(None) \n",
    "\n",
    "                if None not in article:\n",
    "                    articles.append(article)\n",
    "\n",
    "            # Attempt to find the \"Next\" button and exit if there is none\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CLASS_NAME, \"qa-pagination-next-page\")\n",
    "                next_button.click()\n",
    "            except ElementClickInterceptedException:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to CSV function\n",
    "Save the scraped information to CSV for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def write_articles_to_csv(articles, file_name):\n",
    "    # Check if the file already exists\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "\n",
    "    # Open the CSV file in append mode\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = [\"Title\", \"Summary\", \"URL\", \"Interesting\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file doesn't exist, write the header row\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write each article\n",
    "        for article in articles:\n",
    "            writer.writerow({\"Title\": article[0], \"Summary\": article[1], \"URL\": article[2], \"Interesting\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates from CSV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def remove_duplicates_from_csv(file_name):\n",
    "    # Create a set to keep track of seen titles\n",
    "    seen_titles = set()\n",
    "\n",
    "    # Create a temporary file to write the deduplicated data\n",
    "    temp_file_name = file_name + \".tmp\"\n",
    "\n",
    "    with open(file_name, mode='r', newline='', encoding='utf-8') as csv_file, open(temp_file_name, mode='w', newline='', encoding='utf-8') as temp_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        writer = csv.DictWriter(temp_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            title = row[\"Title\"]\n",
    "\n",
    "            # Check if the title has been seen before\n",
    "            if title not in seen_titles:\n",
    "                seen_titles.add(title)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    # Replace the original file with the deduplicated data\n",
    "    os.replace(temp_file_name, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Interesting column\n",
    "Print articles' titles and summaries one by one and fill the \"Interesting\" label based on keyboard input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "def set_interesting(file_name):\n",
    "    try:\n",
    "        with open(file_name, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            fieldnames = reader.fieldnames\n",
    "\n",
    "            # Ensure \"Interesting\" is one of the headers\n",
    "            if \"Interesting\" not in fieldnames:\n",
    "                print(\"'Interesting' column not found in the CSV file.\")\n",
    "                return\n",
    "\n",
    "            # Read all rows into a list\n",
    "            all_rows = list(reader)\n",
    "\n",
    "        for i, row in enumerate(all_rows):\n",
    "            title = row[\"Title\"]\n",
    "            summary = row[\"Summary\"]\n",
    "            interesting = row[\"Interesting\"]\n",
    "\n",
    "            # If \"Interesting\" is already set or if it's not empty, keep the row as is\n",
    "            if interesting and interesting.strip() != \"\":\n",
    "                continue\n",
    "\n",
    "            print(title)\n",
    "            print(summary)\n",
    "\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            try:\n",
    "                choice = input(\"Enter '0' for not interesting or '1' for interesting (or press Enter to skip, Esc to exit): \")\n",
    "\n",
    "                # Check for keyboard interruption (Ctrl+C)\n",
    "                # if choice == '':\n",
    "                #     break\n",
    "                # Check if esc key was pressed\n",
    "                if keyboard.is_pressed('esc'):\n",
    "                    break\n",
    "\n",
    "                # Validate input\n",
    "                if choice not in ('0', '1'):\n",
    "                    print(\"Invalid input. Please enter '0' or '1'.\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Update \"Interesting\" cell\n",
    "                all_rows[i][\"Interesting\"] = choice\n",
    "                print(\"Updated!\\n\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "\n",
    "        # Write all rows back to the new file\n",
    "        with open(file_name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_world_url = \"https://www.bbc.com/news/world\"\n",
    "bbc_science_url = \"https://www.bbc.com/news/science_and_environment\"\n",
    "bbc_tech_url = \"https://www.bbc.com/news/technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_articles_to_csv(scrape_bbcnews(bbc_world_url, 10), 'bbc_world_train.csv')\n",
    "# write_articles_to_csv(scrape_bbcnews(bbc_science_url, 50), 'bbc_science_train.csv')\n",
    "# write_articles_to_csv(scrape_bbcnews(bbc_tech_url, 5), 'bbc_tech_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_duplicates_from_csv('bbc_world_train.csv')\n",
    "# remove_duplicates_from_csv('bbc_science_train.csv')\n",
    "remove_duplicates_from_csv('bbc_tech_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction and truth about today's emergency test\n",
      "False rumours are swirling online about a routine trial of the US alert system.\n",
      "Updated!\n",
      "\n",
      "Couple killed by bear in Canada 'loved the outdoors'\n",
      "A family member said Doug Inglis and Jenny Gusse, both 62, were experienced backcountry hikers.\n",
      "Updated!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_interesting('bbc_world_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
